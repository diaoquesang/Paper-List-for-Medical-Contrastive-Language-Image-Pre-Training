# Med-VLP: A Paper List for Medical Vision-Language Pre-Training

## üëë Universal Methods

### [ICML 2021 - CLIP] [Learning Transferable Visual Models from Natural Language Supervision](https://arxiv.org/abs/2103.00020)

### [ICML 2022 - BLIP] [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://proceedings.mlr.press/v162/li22n.html)

### [arXiv 2023 EVA-CLIP] [EVA-CLIP: Improved Training Techniques for CLIP at Scale](https://arxiv.org/abs/2303.15389)

### [CVPR 2023 OpenCLIP] [Reproducible Scaling Laws for Contrastive Language-Image Learning](https://arxiv.org/abs/2212.07143)

### [CVPR 2023 FLIP] [Scaling Language-Image Pre-training via Masking](http://openaccess.thecvf.com/content/CVPR2023/html/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.html)

### [ICML 2023 BLIP-2] [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://proceedings.mlr.press/v202/li23q)

### [ICCV 2023 SigLIP] [Sigmoid Loss for Language Image Pre-Training](http://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html)

### [ICLR 2024 DFN] [Data Filtering Networks](https://arxiv.org/abs/2309.17425)

### [arXiv 2025 SigLIP 2] [SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://arxiv.org/abs/2502.14786)

### [ICLR 2025 DIVA] [Diffusion Feedback Helps CLIP See Better](https://arxiv.org/abs/2407.20171)

### [ICLR 2026 UniLiP] [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](https://www.arxiv.org/abs/2507.23278)

## üßëüèª‚Äç‚öïÔ∏è Medical-Specific Methods

### [NeurIPS 2022 MGCA] [https://arxiv.org/abs/2210.06044](https://arxiv.org/abs/2210.06044)

### [MICCAI 2022 M3AE] [Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training](https://arxiv.org/abs/2209.07098)

### [EMNLP 2022 MedCLIP] [MedCLIP: Contrastive Learning from Unpaired Medical Images and Text](https://pmc.ncbi.nlm.nih.gov/articles/PMC11323634/)

### [arXiv 2023 BiomedCLIP] [BiomedCLIP: A Multimodal Biomedical Foundation Model Pretrained from Fifteen Million Scientific Image-Text Pairs](https://arxiv.org/abs/2303.00915)

### [Nature Medicine 2023 PLIP] [A Visual‚ÄìLanguage Foundation Model for Pathology Image Analysis Using Medical Twitter](https://arxiv.org/abs/2303.00915)

### [arXiv 2024 CT-CLIP] [Developing Generalist Foundation Models from a Multimodal Dataset for 3D Computed Tomography](https://arxiv.org/abs/2403.17834)

### [arXiv 2024 M3D] [M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models](https://arxiv.org/abs/2404.00578)

### [arXiv 2024 Merlin] [Merlin: A Vision Language Foundation Model for 3D Computed Tomography](https://arxiv.org/abs/2406.06512)

### [arXiv 2025 BrgSA] [Bridged Semantic Alignment for Zero-shot 3D Medical Image Diagnosis](https://arxiv.org/abs/2501.03565)

### [ICLR 2025 fVLM] [Large-Scale and Fine-Grained Vision-Language Pre-Training for Enhanced CT Image Understanding](https://arxiv.org/abs/2501.14548)

### [ICLR 2025 Malenia] [Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment](https://arxiv.org/abs/2410.15744)

### [MedIA 2025] [CLIP in Medical Imaging: A Survey](https://www.sciencedirect.com/science/article/abs/pii/S1361841525000982)
